{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/\n",
    "\n",
    "En el anterior enlace, tenéis un ejemplo sobre cómo, a partir de tweets con un label específico (un sentimiento, positivo o negativo): \n",
    "\n",
    "1. Genera un conjunto de entrenamiento. El conjunto de entrenamiento es formado a partir de tweets completos pasados a un array con un tamaño específico.\n",
    "2. Ese array (X_train de tamaño N) tiene un label que representa el sentimiento (y_train)\n",
    "3. Como todas las frases tienen un tamaño N, la entrada de la red neuronal será de tamaño N y la salida de la red será de tamaño 2 usando activación softmax(porque hay dos clases).\n",
    "\n",
    "Se pide: \n",
    "\n",
    "- Realizar un clasificador de reviews para el dataset de IMDB de la carpeta data_exercise/\n",
    "\n",
    "**Cuando usa la importación \"keras.x\", reemplázalo por \"tensorflow.keras.x\"**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import the necessary libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re \n",
    "import shutil \n",
    "import string \n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import json\n",
    "\n",
    "from datetime import datetime \n",
    "from tensorflow.keras import Model, Sequential \n",
    "from tensorflow.keras.layers import Activation, Dense, Embedding, GlobalAveragePooling1D, Dropout\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import tensorflow.keras.preprocessing.text as kpt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "source": [
    "### Load the IMDb Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "dataset = pd.read_csv('data_exercise/IMDB Dataset/IMDB Dataset.csv', sep=',')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn sentiment column into categorical values\n",
    "\n",
    "le = LabelEncoder()\n",
    "sentiment_le = le.fit_transform(dataset.sentiment)\n",
    "\n",
    "dataset['sentiment_le'] = sentiment_le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X shape: (50000,)\ny shape: (50000,)\n"
     ]
    }
   ],
   "source": [
    "## Divide the data in features and target\n",
    "\n",
    "X = dataset['review']\n",
    "y = dataset['sentiment_le']\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train shape:\t (40000,)\nX_test shape:\t (10000,)\ny_train shape:\t (40000,)\ny_test shape:\t (10000,)\n"
     ]
    }
   ],
   "source": [
    "## Split the data in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "print('X_train shape:\\t', X_train.shape)\n",
    "print('X_test shape:\\t', X_test.shape)\n",
    "print('y_train shape:\\t', y_train.shape)\n",
    "print('y_test shape:\\t', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10000,), dtype=string, numpy=\n",
       "array([b'although this film was made before dogme emerged as the predominant method of filmmaking and before digital triumphed over  strike that you get the point this 1991 masterpiece clearly anticipated those developments corin nemec is just outstanding as the neer do well author and narrator the pace is slow but elegantly so because the cinematography is so beautiful record it the next time its on tv because i guarantee youll never see a better nostalgia ripoff madefor tv movie directtovideo never felt so good',\n",
       "       b'my grandmother took me and my sister out to see this movie when it came out in theaters back in 1998 and so we happily bought the tickets the popcorn and soda and walked right in to the theater and sat down to watch the movie when it was over the audience didnt applauded strongly i remember that i heard a few people say that they didnt like it at all i didnt like it i thought that it was rather stupid and not worth seeing eddie murphy was hysterical in this but apart from him the whole movie was bad i rarely laughed at the parts in this i also remembered that the other people in the theater almost hardly even laughed and what i really thought was bad was making the animals talk because talking animals only exist in cartoons in live action movies they are totally a mutt i said that apart from eddie murphys hysterical twist he brings in this movie is not worth watching it is rather stupid  i have seen eddie murphy in several of movies and i thought that he was funny in those i have just said that he was the only funny part of this movie i also have not seen eddie murphy in the really great movie the adventures of pluto nash this movie is not a movie that i would really recommend that you see because apart from eddie murphy you probably are not going to like this especially because of a lot the the talking animals in it   ill give this movie a rating of 3 stars out of a possible 10 stars',\n",
       "       b'possibly the worst film within the genre in existence it was announced as a comedy but is simply tragically pathetic i dont think anyone could have achieved anything more terrible and irritating if they were specifically requested to it is toilet humour at its very poorest i would avoid even watching the trailer i only went to see it because it was announced that if you like monty python you are bound to love this whoever wrote that was either biased or seriously deranged i am still bewildered how one can honestly believe such a statement rarely do i leave the cinema really it takes a lot of effort for a film to have that effect on me this one did it in just 30 minutes',\n",
       "       ...,\n",
       "       b'my thoughts on the movie 9  it was not good not good at all visually it was great i was pleased with the pacing the camera angles etc however the characters eh kinda bland plot it sucked  this movie seemed more new age crap than anything else organized religion is presented as cowardly and fearful science isnt portrayed any better it creates a monster weapon that kills everything but souls have the power to destroy monsters and bring life really  thats something that bites my ass a bit too here we have a cgi movie created with science and theyre using it to give us the message that science will destroy the world while promoting the idea that spirituality will save us at least they had the decency to have one of the characters ask  okay so now what or something similar i couldnt hear it too well because of the crowd immediately getting up and making a break for the exit it was a okay it was just barely entertaining enough to sit here for the entire movie but now lets get out of here as fast as possible type of exit  this is one of those movies where you cant think if you want to enjoy it just look at the visuals and nod your head prettily any thought as to whats the point of that will suck you out of disbelief and make you eye the exit sign with longing  okay spoilers follow  so basically a scientist creates the machine that is capable of creating other intelligent robotic life evil humans use it as a weapon however the scientist realizes that he is also at fault he gave the machine his intellect but didnt give it his heart   the machine goes skynet on humanitys collective ass and wipes out all life on earth finally slowly powering down however the scientist manages to survive and create walking sockpuppets each one containing a piece of the scientists soul  the last one 9 wakes up not knowing anything about the world he sees a strange device nearby and picks it up he meets up with another like himself 2   well 2 gets captured by a last surviving robot of the machine 9 finds more like himself ands sets off to rescue 2   they succeed  9 notices that there is a matching hole that fits the device perfectly he inserts it and the machine comes back to life pulling out 2s soul in the process  the movie then continues with action scenes with 9 trying to rescue his soulyanked compatriots   they eventually succeed and destroy the machine they release the souls of their fallen friends who go up into the clouds it then rains and we see life returning back to the planet  hunh  that makes no sense none at all why the heck did the scientist want to split his soul into 9 homunculi what did it accomplish were they created to stop the machine everything is dead the machine was dead why bother  why did he expect nine little critters to succeed when nothing else had why not create a second intellectual machine but with a soul to fight the first at least that would have seemed like it would have had a reasonable chance at success  why did they have to have their souls sucked into the device by the machine and then destroy the machine and then release the souls in order to bring life back to earth why not just wait for the machine to power down and bring life back without all the rest of the insane steps',\n",
       "       b'lately they have been trying to hock this film latenight on cable tv commercials dont believe the hype i was one of the unlucky people to see this stinker in theatres this is in my opinion the 3rd worst movie of all time just behind mac  me 1 worst and jack frost 2 worst but i must admit they are all close and all terrible really nothing of this movie is funny or disturbing or anything else it claims to be so dont waste your money the only thing it is good for is giving to your worst enemy im not lying about that someone who you would love to kill or torture would be a prime candidate for this film it is that awful if you dont believe me then you deserve to suffer through the misery of watching this which i doubt you can finish two thumbs enthusiastically down',\n",
       "       b'im actually too drained to write this review  bad movies always do that to me  but i feel obligated as if its my civic duty to warn anyone who might be considering purchasing or viewing this godawful messofapicture please please just take my word this is one youll want to stay away from its so boring and dull so insipid and uninspired such a poor excuse to assemble any familiar talent burt reynolds wasted despite his best efforts icet is barely in the film and when he does appear on screen his performance is so restrained and muted that it becomes crystal clear that the director perhaps intimidated by ices intense stare didnt know what to do with him rob lowe as the title character has never displayed so little onscreen charisma mario van peebles should be ashamed of himself his performance is in the saddest sense possible a joke surely satan himself signed marios check for this film the plot is as weak halfbaked and annoying as all the music involved the utterly boring club song seems to continue on literally for the first third of the movie the films look will prompt one to seriously wonder if the director of photography was also forced like one of the films forgettable female characters to smoke crack from a pipe ducttaped to his mouth and if youre looking at least for stylized shootemuptype violence youll get none here this film i assure you i promise you has absolutely no redeeming qualities please i implore you avoid this flick dont put it in and get suckered into believing that its pace will pick up itll get better and evolve into a decent denouement it wont it dont it cant it sucks now you have been warned and i can now go to bed its 3am  please forgive any resulting errors this admonition might contain  knowing my conscience is clear because ive done my civic duty for my fellow man'],\n",
       "      dtype=object)>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,'[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "custom_standardization(input_data=X_train)\n",
    "custom_standardization(input_data=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delimit the maximum amount of words used for this case\n",
    "max_words = 1000\n",
    "sequence_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "\n",
    "# Feed our text with the tokenizer\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Tokenizers come with a convenient list of words and IDs\n",
    "dictionary = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the result in a dictionary\n",
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_index_array(text):\n",
    "    # one really important thing that `text_to_word_sequence` does\n",
    "    # is make all texts the same length -- in this case, the length\n",
    "    # of the longest text in the set.\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crate an empty list that will be filled with word's indices\n",
    "allWordIndices = []\n",
    "\n",
    "# for each review, change each token to its ID in the Tokenizer's word_index\n",
    "for text in X_train:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After creating the list of words, we have to pass it to an array\n",
    "allWordIndices = np.asarray(allWordIndices)\n",
    "\n",
    "# Last but not least, create one-hot matrices out of the indexed tweets\n",
    "X_train = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, 2)\n",
    "y_train"
   ]
  },
  {
   "source": [
    "## Now it's time to create the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "source": [
    "## Compile the network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 512)               512512    \n_________________________________________________________________\ndropout (Dropout)            (None, 512)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 256)               131328    \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 2)                 514       \n=================================================================\nTotal params: 644,354\nTrainable params: 644,354\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "source": [
    "## Finally, train the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "#Reshape features\n",
    "X_train = X_train.reshape(-1, 1)\n",
    "X_test = X_test.reshape(-1, 1)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
       "       1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "1125/1125 [==============================] - 4s 3ms/step - loss: 0.4567 - accuracy: 0.7752 - val_loss: 0.3225 - val_accuracy: 0.8587\n",
      "Epoch 2/10\n",
      "1125/1125 [==============================] - 3s 3ms/step - loss: 0.3154 - accuracy: 0.8622 - val_loss: 0.3187 - val_accuracy: 0.8625\n",
      "Epoch 3/10\n",
      "1125/1125 [==============================] - 3s 2ms/step - loss: 0.2877 - accuracy: 0.8753 - val_loss: 0.3300 - val_accuracy: 0.8608\n",
      "Epoch 4/10\n",
      "1125/1125 [==============================] - 3s 2ms/step - loss: 0.2370 - accuracy: 0.8975 - val_loss: 0.3291 - val_accuracy: 0.8610\n",
      "Epoch 5/10\n",
      "1125/1125 [==============================] - 3s 2ms/step - loss: 0.1861 - accuracy: 0.9191 - val_loss: 0.3817 - val_accuracy: 0.8565\n",
      "Epoch 6/10\n",
      "1125/1125 [==============================] - 3s 2ms/step - loss: 0.1321 - accuracy: 0.9448 - val_loss: 0.4014 - val_accuracy: 0.8633\n",
      "Epoch 7/10\n",
      "1125/1125 [==============================] - 3s 2ms/step - loss: 0.1009 - accuracy: 0.9593 - val_loss: 0.3964 - val_accuracy: 0.8565\n",
      "Epoch 8/10\n",
      "1125/1125 [==============================] - 3s 2ms/step - loss: 0.0831 - accuracy: 0.9669 - val_loss: 0.4862 - val_accuracy: 0.8660\n",
      "Epoch 9/10\n",
      "1125/1125 [==============================] - 3s 3ms/step - loss: 0.0630 - accuracy: 0.9747 - val_loss: 0.5106 - val_accuracy: 0.8568\n",
      "Epoch 10/10\n",
      "1125/1125 [==============================] - 3s 3ms/step - loss: 0.0639 - accuracy: 0.9759 - val_loss: 0.5579 - val_accuracy: 0.8562\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26134116a60>"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "  epochs=10,\n",
    "  verbose=1,\n",
    "  validation_split=0.1,\n",
    "  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for human-friendly printing\n",
    "labels = ['negative', 'positive']\n",
    "\n",
    "# this utility makes sure that all the words in your input\n",
    "# are registered in the dictionary\n",
    "# before trying to turn them into a matrix.\n",
    "def convert_text_to_index_array(text):\n",
    "    words = kpt.text_to_word_sequence(text)\n",
    "    wordIndices = []\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            wordIndices.append(dictionary[word])\n",
    "        else:\n",
    "            print(\"'%s' not in training corpus; ignoring.\" %(word))\n",
    "    return wordIndices\n",
    "\n",
    "# read in your saved model structure\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "# and create a model from that\n",
    "model = model_from_json(loaded_model_json)\n",
    "# and weight your nodes with your saved values\n",
    "model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay here's the interactive part\n",
    "while True:\n",
    "    sentence = input('Input a sentence to be evaluated, or Enter to quit: ')\n",
    "\n",
    "    if len(sentence) == 0:\n",
    "        break\n",
    "\n",
    "    # format your input for the neural net\n",
    "    testArr = convert_text_to_index_array(sentence)\n",
    "    final_input = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "    # predict which bucket your input belongs in\n",
    "    pred = model.predict(final_input)\n",
    "    # and print it for the humons\n",
    "    print(sentence)\n",
    "    print(\"%s sentiment; %f%% confidence\" % (labels[np.argmax(pred)], pred[0][np.argmax(pred)] * 100))"
   ]
  }
 ]
}